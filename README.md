<!-- markdownlint-disable MD033 -->
<div align="center">

# YourNovelHelper

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![Qwen](https://img.shields.io/badge/Model-Qwen3--4B-0a0a0a?style=flat&logo=Qwen)](https://github.com/QwenLM/Qwen2.5)

*åŸºäº Qwen3-4B çš„å°è¯´é£æ ¼å¾®è°ƒé¡¹ç›®ï¼Œå¸®åŠ©ç”¨æˆ·åˆ›å»ºè‡ªå·±å–œæ¬¢é£æ ¼çš„å°è¯´*

</div>

---

## åŠŸèƒ½ç‰¹æ€§

| åŠŸèƒ½ | æè¿° |
|:---:|:---|
| ğŸ“š **æ•°æ®é¢„å¤„ç†** | å°†åŸå§‹å°è¯´æ–‡æœ¬è½¬æ¢ä¸ºè®­ç»ƒæ•°æ® |
| ğŸ”§ **LoRA å¾®è°ƒ** | ä½¿ç”¨ QLoRA ä½æˆæœ¬å¾®è°ƒ Qwen3-4B æ¨¡å‹ |
| ğŸ’» **å‘½ä»¤è¡Œå·¥å…·** | äº¤äº’å¼å°è¯´ç»­å†™ |
| ğŸŒ **Web API** | FastAPI æœåŠ¡æ¥å£ |
| ğŸ¨ **Web UI** | Gradio å›¾å½¢ç•Œé¢ |

---

## é¡¹ç›®ç»“æ„

```
YourNovelHelper/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml          # é…ç½®æ–‡ä»¶
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ processed/            # å¤„ç†åæ•°æ®
â”‚   â””â”€â”€ output/               # è¾“å‡ºç›®å½•
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ preprocess.py     # æ•°æ®é¢„å¤„ç†
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ train.py          # æ¨¡å‹è®­ç»ƒ
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â””â”€â”€ generate.py       # æ¨ç†ç”Ÿæˆ
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ main.py           # FastAPI æœåŠ¡
â”‚       â””â”€â”€ webui.py          # Gradio Web UI
â”œâ”€â”€ models/                   # æ¨¡å‹å­˜å‚¨
â”œâ”€â”€ logs/                     # æ—¥å¿—
â””â”€â”€ scripts/                  # è„šæœ¬
```

---

## é¡¹ç›®æµç¨‹

```mermaid
flowchart TD
    subgraph æ•°æ®å‡†å¤‡
        A1[åŸå§‹å°è¯´æ•°æ®] --> A2[æ•°æ®é¢„å¤„ç†<br/>src/data/preprocess.py]
        A2 --> A3[train.jsonl<br/>val.jsonl<br/>test.jsonl]
    end

    subgraph æ¨¡å‹è®­ç»ƒ
        A3 --> B1[åŠ è½½é¢„è®­ç»ƒæ¨¡å‹<br/>Qwen3-4B]
        B1 --> B2[é…ç½®LoRA/QLoRA]
        B2 --> B3[åŠ è½½è®­ç»ƒæ•°æ®]
        B3 --> B4[æ‰§è¡Œè®­ç»ƒ<br/>src/training/train.py]
        B4 --> B5[LoRA Checkpoint]
        B5 --> B6[åˆå¹¶å¯¼å‡ºæ¨¡å‹<br/>models/novel-qlora]
    end

    subgraph æ¨¡å‹ä½¿ç”¨
        B6 --> C1[å‘½ä»¤è¡Œå·¥å…·<br/>src/inference/generate.py]
        B6 --> C2[FastAPIæœåŠ¡<br/>src/api/main.py]
        B6 --> C3[Web UIç•Œé¢<br/>src/api/webui.py]
    end

    style A1 fill:#e1f5fe
    style B1 fill:#e8f5e9
    style C1 fill:#fff3e0
```

---

## å¿«é€Ÿå¼€å§‹

### 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–

æ¨èä½¿ç”¨ [uv](https://github.com/astral-sh/uv) ç®¡ç† Python ç¯å¢ƒï¼š

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
uv venv

# æ¿€æ´»ç¯å¢ƒ (Linux/Mac)
source .venv/bin/activate

# Windows
# .venv\Scripts\activate

# å®‰è£…ä¾èµ–
uv pip install torch transformers peft datasets trl accelerate pyyaml
uv pip install fastapi uvicorn gradio jieba tqdm scikit-learn
uv pip install modelscope
```

æˆ–è€…å®‰è£…é¡¹ç›®ï¼ˆåŒ…å«æ‰€æœ‰ä¾èµ–ï¼‰ï¼š

```bash
uv pip install -e .
```

> **æ³¨æ„**: å¦‚æœæ²¡æœ‰ uvï¼Œè¯·å…ˆå®‰è£…: `pip install uv`

> æ¿€æ´»ç¯å¢ƒåï¼Œåç»­å‘½ä»¤å¯ä»¥ç›´æ¥ä½¿ç”¨ `python` è¿è¡Œã€‚

### 2. å‡†å¤‡æ•°æ®

å°†å°è¯´æ–‡æœ¬æ–‡ä»¶æ”¾å…¥ `data/raw/` ç›®å½•ï¼Œæ”¯æŒæ ¼å¼:
- `.txt` æ–‡ä»¶
- `.json` æ–‡ä»¶ (åŒ…å« `text` å­—æ®µæˆ– `texts` æ•°ç»„)

### 3. æ•°æ®é¢„å¤„ç†

```bash
python -m src.data.preprocess --raw-dir data/raw --output-dir data/processed
```

### 4. è®­ç»ƒæ¨¡å‹

```bash
python -m src.training.train
```

è®­ç»ƒå‚æ•°å¯åœ¨ `config/config.yaml` ä¸­ä¿®æ”¹ã€‚

### 5. ä½¿ç”¨æ¨¡å‹

> **æ³¨æ„**: é»˜è®¤ä½¿ç”¨ ModelScope åŠ è½½ Qwen3-4B æ¨¡å‹ã€‚å¦‚éœ€ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼Œå¯åœ¨å‘½ä»¤ä¸­æŒ‡å®šã€‚

#### å‘½ä»¤è¡Œ

```bash
# äº¤äº’æ¨¡å¼
python -m src.inference.generate --interactive

# å•æ¬¡ç”Ÿæˆ
python -m src.inference.generate --prompt "æ¸…æ™¨çš„é˜³å…‰é€è¿‡çª—æˆ·"
```

#### Web API

```bash
python -m src.api.main
```

è®¿é—® http://localhost:8000/docs æŸ¥çœ‹ API æ–‡æ¡£ã€‚

#### Web UI

```bash
python -m src.api.webui
```

è®¿é—® http://localhost:7860 æ‰“å¼€ Web ç•Œé¢ã€‚

---

## ç»Ÿä¸€è¿è¡Œå…¥å£

é¡¹ç›®æä¾› `run.sh` ç»Ÿä¸€å…¥å£ï¼Œæ”¯æŒ CLI å’Œ Docker ä¸¤ç§è¿è¡Œæ¨¡å¼ã€‚

### ä½¿ç”¨æ–¹å¼

```bash
./run.sh <cli|docker> <command> [options]
```

### å¯ç”¨å‘½ä»¤

| å‘½ä»¤ | è¯´æ˜ |
|:---|:---|
| `download` | ä» ModelScope ä¸‹è½½æ¨¡å‹ |
| `preprocess` | é¢„å¤„ç†å°è¯´æ•°æ®é›† |
| `train` | å¾®è°ƒæ¨¡å‹ |
| `generate` | æ–‡æœ¬è¡¥å…¨ |

### ç¤ºä¾‹

```bash
# CLI æ¨¡å¼ï¼ˆå®¿ä¸»æœºè¿è¡Œï¼‰
./run.sh cli download --model Qwen3-4B --output ./models
./run.sh cli preprocess --raw-dir ./data/raw --output-dir ./data/processed
./run.sh cli train --model-dir ./models --data-dir ./data/processed --output-dir ./models/checkpoints --logs-dir ./logs
./run.sh cli generate --model-dir ./models --base-model Qwen3-4B --lora ./models/novel-qlora --prompt "å°è¯´å¼€å¤´"

# Docker æ¨¡å¼ï¼ˆå®¹å™¨å†…è¿è¡Œï¼‰
./run.sh docker download --model Qwen3-4B --output ./models
./run.sh docker preprocess --raw-dir ./data/novels --output-dir ./data/processed
./run.sh docker train --model-dir /app/models --data-dir /app/data/processed --output-dir /app/models/checkpoints --logs-dir /app/logs
./run.sh docker generate --model-dir /app/models --base-model Qwen3-4B --prompt "å¼€å¤´"
```

è¯¦ç»†å‚æ•°è¯·è¿è¡Œ `./run.sh` æŸ¥çœ‹å¸®åŠ©ã€‚

---

## Docker éƒ¨ç½²

### 1. ä¸‹è½½æ¨¡å‹ï¼ˆæ¨èï¼‰

æ¨¡å‹ç›®å½•ç»“æ„ï¼š
```
models/
â”œâ”€â”€ Qwen3-4B/          # åŸºç¡€æ¨¡å‹
â”œâ”€â”€ Qwen3-7B/           # å¯é€‰å…¶ä»–æ¨¡å‹
â””â”€â”€ novel-qlora/        # è®­ç»ƒåçš„ LoRA
```

ä¸‹è½½æ¨¡å‹ï¼š
```bash
# ä¸‹è½½ Qwen3-4B (é»˜è®¤ï¼Œçº¦ 8GB)
python scripts/download_model.py

# æŸ¥çœ‹å¯ä¸‹è½½çš„æ¨¡å‹
python scripts/download_model.py --list
```

### 2. æ„å»ºé•œåƒ

```bash
docker build . --tag your-novel-helper:latest
```

### 3. è¿è¡Œå®¹å™¨

```bash
# ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç›®å½•
docker run -d -p 8000:8000 \
  -v $(pwd)/models:/app/models \
  -e MODEL_DIR=/app/models \
  -e BASE_MODEL=Qwen3-4B \
  -e LORA_NAME=novel-qlora \
  your-novel-helper:latest

# æˆ–ä½¿ç”¨ ModelScope è‡ªåŠ¨ä¸‹è½½
docker run -d -p 8000:8000 your-novel-helper:latest

# è¿è¡Œ Web UI
docker run -d -p 7860:7860 \
  -v $(pwd)/models:/app/models \
  -e MODEL_DIR=/app/models \
  -e BASE_MODEL=Qwen3-4B \
  your-novel-helper:latest python -m src.api.webui
```

### 4. ä½¿ç”¨ Docker Compose

åˆ›å»º `docker-compose.yml`:

```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models    # æŒ‚è½½æ¨¡å‹ç›®å½•
      - ./data:/app/data        # æŒ‚è½½æ•°æ®
    environment:
      - MODEL_DIR=/app/models
      - BASE_MODEL=Qwen3-4B
      - LORA_NAME=novel-qlora
      - PYTHONUNBUFFERED=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

è¿è¡Œ:

```bash
docker-compose up -d
```

### ç¯å¢ƒå˜é‡è¯´æ˜

| å˜é‡ | è¯´æ˜ | é»˜è®¤å€¼ |
|:---|:---|:---|
| `MODEL_DIR` | æ¨¡å‹ç›®å½•ï¼ˆåŒ…å«å¤šä¸ªæ¨¡å‹å­ç›®å½•ï¼‰ | `models` |
| `BASE_MODEL` | åŸºç¡€æ¨¡å‹åç§°ï¼ˆmodel_dir ä¸‹çš„å­ç›®å½•åï¼‰ | `Qwen3-4B` |
| `LORA_NAME` | LoRA æ¨¡å‹åç§°ï¼ˆmodel_dir ä¸‹çš„å­ç›®å½•åï¼‰ | `` |
| `CONFIG_PATH` | é…ç½®æ–‡ä»¶è·¯å¾„ | `config/config.yaml` |

### CLI ä½¿ç”¨æ–¹å¼

```bash
# ä½¿ç”¨æœ¬åœ°æ¨¡å‹
python -m src.inference.generate \
  --model-dir models \
  --base-model Qwen3-4B \
  --lora novel-qlora \
  --prompt "æ¸…æ™¨çš„é˜³å…‰"

# äº¤äº’æ¨¡å¼
python -m src.inference.generate \
  --model-dir models \
  --base-model Qwen3-4B \
  --interactive
```

---

## é…ç½®è¯´æ˜

`config/config.yaml` ä¸»è¦é…ç½®é¡¹:

| é…ç½®é¡¹ | è¯´æ˜ | é»˜è®¤å€¼ |
|:---|:---|:---|
| `model.name` | æ¨¡å‹åç§° (æ”¯æŒ ModelScope æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾„) | Qwen3-4B |
| `training.method` | è®­ç»ƒæ–¹æ³• | qlora |
| `training.lora_rank` | LoRA rank | 16 |
| `training.num_epochs` | è®­ç»ƒè½®æ•° | 3 |
| `inference.temperature` | ç”Ÿæˆæ¸©åº¦ | 0.7 |
| `api.port` | API æœåŠ¡ç«¯å£ | 8000 |

---

## ç¡¬ä»¶è¦æ±‚

| åœºæ™¯ | æœ€ä½è¦æ±‚ |
|:---|:---|
| **è®­ç»ƒ** | 8GB æ˜¾å­˜ (QLoRA) |
| **æ¨ç†** | 6GB æ˜¾å­˜ |

---

## ç¤ºä¾‹

```python
from src.inference.generate import NovelGenerator

# ä½¿ç”¨é»˜è®¤æ¨¡å‹ (Qwen3-4B from ModelScope)
generator = NovelGenerator()
result = generator.generate(
    prompt="é›¨å¤œï¼ŒåŸå¸‚çš„ä¸€è§’",
    style_prompt="é‡‘åº¸çš„æ­¦ä¾ é£æ ¼"
)
print(result)
```

---

## API ç¤ºä¾‹

```bash
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "ä¸»äººå…¬èµ°åœ¨è¡—ä¸Š",
    "style": "æ‚¬ç–‘æ¨ç†",
    "max_new_tokens": 1000
  }'
```

---

## è®¸å¯è¯

MIT License

---

> æœ¬é¡¹ç›®ç”± [OpenCode](https://opencode.ai) AI ç¼–ç¨‹åŠ©æ‰‹ååŠ©å¼€å‘ã€‚
